{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1.1: Exploring WS and BA models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Study of the Watts-Stogatz(WS) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question related to the given reading material for part 1 (Network Science Chapter 3, Section 3.5 - 3.10, with emphasis on 3.8 and 3.9.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** *What's the problem with random networks as a model for real-world networks according to the argument in section 3.5 (near the end)?*\n",
    "\n",
    "**Answer 1:** The argument near the end of section 3.5 is that the random network model underestimates the size and frequency of both high- and low-degree nodes. This is illustrated in Image 3.6, where the random network peaks at $<k>$, while in real life the probability is higher at the beginning (i.e., for low-degree nodes) and extends further into the tail (i.e., for high-degree nodes).\n",
    "\n",
    "In more practical terms, as described in the chapter, this means that in a random network model there are neither highly popular individuals nor people with very few connections. This is not reflective of reality, as the chapter notes with examples such as U.S. President Franklin Delano Roosevelt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** *List the four regimes that characterize random networks as a function of  ‚ü®k‚ü©*\n",
    "\n",
    "**Answer 2:** The four regimes which characterizes random networks as a function of ‚ü®k‚ü© are:\n",
    "1. The Subcritical Regime\n",
    "2. The Critical Point\n",
    "3. The Supercritical Regime\n",
    "4. The Connected Regime\n",
    "\n",
    "The subcritical regime consist of many small components, since there are not that many links formed, and it is not quite possible to determine a component to be the giant component.\n",
    "\n",
    "The critical point explains the point which separates the subcritical regime from the supercritical regime. Most nodes are still located in numerous small components, however clusters of different sizes coexists. \n",
    "\n",
    "The supercritical regime explains the stage where one giant component is designated, while some nodes still exists in smaller components. This stage will last until all the smaller components are absorbed by the giant component, and by then the network will become a connected regime. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3** *According to the book, why is it a problem for random networks (in terms of being a model for real-world networks) that the degree-dependent clustering  C(k) decreases as a function of  k in real-world networks?*\n",
    "\n",
    "**Answer 3** It becomes a problem because degree-depending clustering C(k) is as explained by its name, dependent on the degree, hence a function of k in real-life networks, where the clustering in random networks is dependent on the size of the system. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WS edition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries (networkx for creating the small network and using built-in function for their properties, numpy for calculating statistical properties, and matplotlib for plotting)\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Using the Watts-Strogatz model to generate graphs and analyze their properties, such as average shortest path length. \n",
    "# watts_strogatz_graph is given as watts_strogatz_graph(n=The number of nodes, k, p=probability of rewiring each edge)\n",
    "graph_1 = nx.watts_strogatz_graph(500, 4, 0)\n",
    "graph_2 = nx.watts_strogatz_graph(500, 4, 0.1)\n",
    "graph_3 = nx.watts_strogatz_graph(500, 4, 1)\n",
    "\n",
    "# Calculate average shortest path length for each graph, given the built-in function average_shortest_path_length.\n",
    "avg_path_length_g1 = nx.average_shortest_path_length(graph_1)\n",
    "avg_path_length_g2 = nx.average_shortest_path_length(graph_2)\n",
    "avg_path_length_g3 = nx.average_shortest_path_length(graph_3)\n",
    "\n",
    "#Print the results\n",
    "print(f\"Average shortest path length for p=0: {avg_path_length_g1}\")\n",
    "print(f\"Average shortest path length for p=0.1: {avg_path_length_g2}\")\n",
    "print(f\"Average shortest path length for p=1: {avg_path_length_g3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When p goes towards 1, it can be seen that the network becomes more intertwined, and hence the shortest path lengths becomes shorter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support the conclusion, we can plot the average shortest path length as a function of the rewiring probability p.\n",
    "# We will calculate the average shortest path length for different values of p, and plot the results\n",
    "\n",
    "# Define rewiring probabilities to test\n",
    "p_values = [0, 0.01, 0.03, 0.05, 0.1, 0.2]\n",
    "\n",
    "# Store average shortest path lengths and their standard deviations\n",
    "avg_path_lengths = []\n",
    "std_dev_path_lengths = []\n",
    "\n",
    "# For each p, generate multiple graphs, calculate their average shortest path lengths, and compute the mean and standard deviation\n",
    "for p in p_values:\n",
    "    #path_lengths for current p\n",
    "    path_lengths = []\n",
    "    \n",
    "    #inner loop to average out the randomness, generating 50 graphs for each p\n",
    "    for _ in range(50):\n",
    "        graph = nx.watts_strogatz_graph(500, 4, p)\n",
    "        path_length = nx.average_shortest_path_length(graph)\n",
    "        path_lengths.append(path_length)\n",
    "\n",
    "    # Calculate mean and standard deviation for current p\n",
    "    avg_path_lengths.append(np.mean(path_lengths))\n",
    "    std_dev_path_lengths.append(np.std(path_lengths))\n",
    "\n",
    "# Plotting the results with error bars\n",
    "plt.errorbar(p_values, avg_path_lengths, yerr=std_dev_path_lengths, fmt='-o')\n",
    "\n",
    "#Add labels and title\n",
    "plt.xlabel('Rewiring Probability (p)')\n",
    "plt.ylabel('Average Shortest Path Length ‚ü®d‚ü©')\n",
    "plt.title('Average Shortest Path Length per Rewiring Probability')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure Caption:** Average shortest path length ‚ü®d‚ü© as a function of the rewiring probability ùëù in Watts‚ÄìStrogatz networks with ùëÅ=500 nodes and average degree ‚ü®ùëò‚ü©=4. Each data point represents the mean ‚ü®d‚ü© over 50 independently generated networks, with error bars showing the standard deviation. The results demonstrate that even a very small amount of rewiring (e.g., ùëù‚âà0.01) leads to a significant decrease in path length. The decrease is especially steep at the beginning, after which the reduction slows down and ‚ü®d‚ü© gradually approaches the short paths characteristic of a fully randomized network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Study of the and Barabasi-Albert(BA) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1** *What are the three slope dependent regimes of complex networks with power-law degree distributions? Briefly describe each one. (You will have to skim chp 4.7 to answer this one)*\n",
    "\n",
    "**Answer 1:** The three slope dependent regimes of complex networks with power-law degree distributions are:\n",
    "1. Anomalous Regime $(y \\leq 2)$\n",
    "2. Scale-Free Regime $(2 < y < 3)$\n",
    "3. Random Network Regime $(y > 3)$\n",
    "\n",
    "where y determines the degree exponent.\n",
    "\n",
    "In the Anomalous Regime the exponent, $1/(y‚àí 1)$, will be greater than one, hence the networks size increases slower than the number of links connected to the largest hub. This means that for a network of a sufficient large N, the hub will run out of nodes to connect to, hence large-scale networks in the Anomalous regime cannot exists if they lack multi-links.\n",
    "\n",
    "Networks in the Scale-Free Regime are ultra-small world networks. As the network grows kmax will grow as well, but with an exponent smaller than 1, since for $2 < y < 3$, it will be such that $1< 1/(y‚àí 1) \\leq 1/2$.\n",
    "\n",
    "It is hard to distinguish the properties of scale free networks in the random network regime from the properties of a random network of similar size. The average distance between the nodes converges to the small-world formula derived for random networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** *What are the three regimes we find in non-linear preferential attachment? (chapter 5) Briefly describe each one.* \n",
    "\n",
    "**Answer 2:** The three regimes found in the non-linear preferential attachment are:\n",
    "1. Sublinear Regime $(0 < \\alpha < 1)$\n",
    "2. Linear Regime  $(\\alpha =1)$\n",
    "3. Superlinear Regime $(\\alpha  > 1)$\n",
    "\n",
    "In the sublinear regime the degree distribution of the network follows the stretched exponential, hence the network consists of fewer and smaller hubs than a scale free network, and is better described as a random network. This happens due to the fact that $0 < \\alpha$ results in new nodes favoring the more connected nodes, however since $\\alpha  < 1$ the bias between the different nodes is weak. \n",
    "\n",
    "The linear regimes determines the regime where the Preferential Attachment is given as $\\alpha  = 1$, which corresponds to the Barab√°si-Albert model, hence the degree distribution follows a power law. \n",
    "\n",
    "In the Superlinear Regime $(\\alpha  > 1)$, the growth of the network follows a winner-takes-all dynamic or rich gets richer process. The bias of the nodes with many connection is weighted highly, and a new node is more likely to choose this node. Hence in this regime the firsts node to occur in the system will become super hubs, and all other nodes will link to these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section will be the group following *\"the recipe for success\"*, building a Barabasi-Albert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# First create a graph consisting of a single link.\n",
    "G = nx.Graph()\n",
    "G.add_edge(0, 1)\n",
    "\n",
    "# Grow the network to 100 nodes, one at a time\n",
    "while len(G.nodes) < 100:\n",
    "    new_node = len(G.nodes)\n",
    "    \n",
    "    # Flatten the edge list to get nodes proportional to degree\n",
    "    edge_list = list(G.edges())\n",
    "    flattened = [node for edge in edge_list for node in edge]\n",
    "    \n",
    "    # Choose one node to connect to, based on degree\n",
    "    target_node = random.choice(flattened)\n",
    "    \n",
    "    # Add the new node and connect it\n",
    "    G.add_edge(new_node, target_node)\n",
    "\n",
    "# Then plot the network\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw(G, with_labels=True, node_size=100, node_color='lightCoral', edge_color='gray')\n",
    "plt.title(\"Custom Barab√°si‚ÄìAlbert Network (100 nodes)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it is time to grow the network to 5000 nodes\n",
    "while len(G.nodes) < 5000:\n",
    "    new_node = len(G.nodes)\n",
    "    \n",
    "    # Flatten the edge list to get nodes proportional to degree\n",
    "    edge_list = list(G.edges())\n",
    "    flattened = [node for edge in edge_list for node in edge]\n",
    "    \n",
    "    # Choose one node to connect to, based on degree\n",
    "    target_node = random.choice(flattened)\n",
    "    \n",
    "    # Add the new node and connect it\n",
    "    G.add_edge(new_node, target_node)\n",
    "\n",
    "# Now the network contains 5000 nodes. With the new network, the  maximum and minimum degree is calculated.\n",
    "# Gets the degrees of all nodes\n",
    "degrees = [deg for _, deg in G.degree()] \n",
    "\n",
    "# Find max and min degree\n",
    "max_degree = max(degrees)\n",
    "min_degree = min(degrees)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Max degree: {max_degree}\")\n",
    "print(f\"Min degree: {min_degree}\")\n",
    "\n",
    "# Then the degrees distribution is getting into bins using numpy.histogram.\n",
    "# Bin the degree distribution\n",
    "counts, bin_edges = np.histogram(degrees, bins=range(min(degrees), max(degrees) + 2))\n",
    "bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "\n",
    "# Filter out zero counts for log-log plot (step made as of begugging)\n",
    "nonzero_indices = counts > 0\n",
    "bin_centers = bin_centers[nonzero_indices]\n",
    "counts = counts[nonzero_indices]\n",
    "\n",
    "#Plot of the distribution, with both linear and log-log axes.\n",
    "\n",
    "# Plot: Linear scale \n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(bin_centers, counts, color='lightCoral')\n",
    "plt.title(\"Degree Distribution (Linear Scale)\")\n",
    "plt.xlabel(\"k (Degree)\")\n",
    "plt.ylabel(\"Frequency (count)\")\n",
    "\n",
    "# Plot: Log-log scale \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(bin_centers, counts, color='gray', s=30)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"k (Degree)\")\n",
    "plt.ylabel(\"Frequency (count)\")\n",
    "plt.title(\"Degree Distribution (Log-Log Scale)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1.2: Stats and visualization of the Rock Music Network\n",
    "\n",
    "This second part requires you to have built the network of Rock Musicians as described in the exercises for Week 4. You should complete the following exercise from **Part 2**.\n",
    "\n",
    "* *Explain your process in words*\n",
    "\n",
    "* *Simple network statistics and analysis*.\n",
    "\n",
    "  * **Note related to this and the following exercise**. It is nice to have the dataset underlying the statistics and visualization available when we grade. Therefore, I recommend that you create a small *network dataset*, which is simply your graph stored in some format that you like (since it's only a few hundred nodes and a few thousand edges, it won't take up a lot of space). You can then place that network one of your group members' GitHub account (or some other server that's available online) and have your Jupyter Notebook fetch that dataset when it runs. (It's OK to use an LLM for help with setting this up, if it seems difficult). \n",
    "\n",
    "And the following exercise from **Part 3**\n",
    "\n",
    "* *Let's build a simple visualization of the network*\n",
    "\n",
    "And that's it! You're all set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import urllib.request\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import networkx as nx\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters (like in the video)\n",
    "baseurl   = \"https://en.wikipedia.org/w/api.php?\"\n",
    "action    = \"action=query\"\n",
    "#title     = \"titles=Aerosmith\"   # <--- for testing purposes\n",
    "title     = \"titles=List_of_mainstream_rock_performers\"\n",
    "content   = \"prop=revisions&rvprop=content&rvslots=main\"\n",
    "datafmt   = \"format=json\"\n",
    "\n",
    "# construct the query (string formatting like in the video)\n",
    "query = \"%s%s&%s&%s&%s\" % (baseurl, action, title, content, datafmt)\n",
    "\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "req = urllib.request.Request(query, headers=headers)\n",
    "\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    data = json.loads(response.read().decode(\"utf-8\"))\n",
    "\n",
    "pages = data[\"query\"][\"pages\"]\n",
    "page  = next(iter(pages.values()))\n",
    "\n",
    "rev   = page[\"revisions\"][0]\n",
    "\n",
    "# Debugging criteria, as the wikitext  wikitext content can appear in two formats (old/new API)\n",
    "if \"*\" in rev:\n",
    "    wikitext = rev[\"*\"]\n",
    "else:\n",
    "    wikitext = rev[\"slots\"][\"main\"][\"*\"]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  apply regex to find all wiki-links \n",
    "pattern = r'\\[\\[([^\\]|#]+)(?:\\|([^\\]]+))?\\]\\]'\n",
    "pairs   = re.findall(pattern, wikitext)\n",
    "\n",
    "#  clean up the names (skip categories etc.) \n",
    "names = []\n",
    "seen  = set()\n",
    "\n",
    "for target, display in pairs:\n",
    "    if \":\" in target:            # skip Category:, File:, Template:\n",
    "        continue\n",
    "    name = display or target     # use display text if present\n",
    "    if name not in seen:\n",
    "        seen.add(name)\n",
    "        names.append(name)\n",
    "\n",
    "print(len(names), \"performer names found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the output folder exists\n",
    "outdir = \"performers\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "# Keep a list of saved filenames\n",
    "saved_files = []\n",
    "\n",
    "# Loop over all names/bands/artists\n",
    "for name in names:\n",
    "    #  Build the Wikipedia title \n",
    "    title_raw = name.replace(\" \", \"_\")            # underscores instead of spaces\n",
    "    title_enc = urllib.parse.quote(title_raw)     # URL encode special chars\n",
    "\n",
    "    # Change a input in the API query string\n",
    "    title     = \"titles=\" + title_enc\n",
    "    query     = \"%s%s&%s&%s&%s\" % (baseurl, action, title, content, datafmt)\n",
    "\n",
    "    #  Fetch the data from Wikipedia using new query\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    req = urllib.request.Request(query, headers=headers)\n",
    "\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        data = json.loads(response.read().decode(\"utf-8\"))\n",
    "\n",
    "    #  Extract the wikitext \n",
    "    pages = data[\"query\"][\"pages\"]\n",
    "    page  = next(iter(pages.values()))\n",
    "    rev   = page[\"revisions\"][0]\n",
    "    if \"*\" in rev:\n",
    "        wikitext = rev[\"*\"]\n",
    "    else:\n",
    "        wikitext = rev[\"slots\"][\"main\"][\"*\"]\n",
    "    \n",
    "    #  Sanitize the filename (replace /, :, etc.) \n",
    "    safe_filename = re.sub(r'[\\\\/*?:\"<>|]', \"_\", title_raw)\n",
    "    filepath = os.path.join(outdir, safe_filename + \".txt\")\n",
    "\n",
    "    #  Save the file (wikitext already fetched in earlier cells)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(wikitext)\n",
    "\n",
    "    print(\"Saved:\", filepath)\n",
    "    saved_files.append(filepath)   # remember the file\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nTotal files saved:\", len(saved_files))\n",
    "print(\"First few:\", saved_files[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all performer names from the saved files in the \"performers\" directory\n",
    "files = [f for f in os.listdir(outdir) if f.endswith(\".txt\")]\n",
    "performers = [f[:-4] for f in files]\n",
    "performer_set = set(performers)\n",
    "\n",
    "# Make a directed graph and add all nodes up-front\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(performers)\n",
    "\n",
    "# Print some stats, to ensure all artist have been added\n",
    "print(\"nodes in graph:\", G.number_of_nodes())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to create the network and add negihbours to each artist. This is done be looping trough each artist, opening their related file and then extracting the neighbors. Afterwards, for each link, neighbor, then the name of the neighbors is extracted. Then to ensure that we only add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex to capture the target part of [[Target]] or [[Target|Display]]\n",
    "link_pattern       = r'\\[\\[([^\\]|#]+)(?:\\|[^\\]]+)?\\]\\]'  # [[Target]] or [[Target|Display]]\n",
    "\n",
    "edge_count = 0\n",
    "\n",
    "for file in files:\n",
    "    # node id (same as filename stem)\n",
    "    source = file[:-4]                         \n",
    "    path   = os.path.join(outdir, file)\n",
    "\n",
    "    # read saved wikitext\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        wikitext = f.read()\n",
    "\n",
    "    # store length of content as node attribute\n",
    "    G.nodes[source][\"word_count\"] = len(wikitext)\n",
    "\n",
    "    # link extraction\n",
    "    links = re.findall(link_pattern, wikitext)\n",
    "    neighbors = []\n",
    "    # for each neighbors\n",
    "    for t in links:\n",
    "        t = t.strip()                  # clean whitespace\n",
    "        if \":\" in t:                   # skip namespaces\n",
    "            continue\n",
    "        t = t.replace(\" \", \"_\")        # match saved filename convention\n",
    "        t_safe = re.sub(r'[\\\\/*?:\"<>|]', \"_\", t)  # same sanitization as when saving\n",
    "        \n",
    "        # keep only links that point to another performer (and not self-loops)\n",
    "        if t_safe in performer_set and t_safe != source:\n",
    "            neighbors.append(t_safe)\n",
    "            G.add_edge(source, t_safe)\n",
    "            edge_count += 1\n",
    "\n",
    "print(\"done. nodes:\", G.number_of_nodes(), \"| edges:\", G.number_of_edges(), \"(added:\", edge_count, \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code pulls the existing network from a online github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling the network from github\n",
    "\n",
    "base_url =  \"https://raw.githubusercontent.com/AlexJHage/Rock-band-network/main/rock_bands.gexf\"\n",
    "\n",
    "response = urllib.request.urlopen(base_url).read()\n",
    "\n",
    "G = nx.read_gexf(io.BytesIO(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolated nodes before: 84\n",
      "Total nodes before removing isolates: 485\n",
      "Total nodes after removing isolates: 401\n"
     ]
    }
   ],
   "source": [
    "# Remove isolated nodes (nodes with no edges)\n",
    "print(\"Isolated nodes before:\", nx.number_of_isolates(G))\n",
    "\n",
    "# print total numer of nodes (conformation that the code will do as intended)\n",
    "print(\"Total nodes before removing isolates:\", G.number_of_nodes())\n",
    "\n",
    "# Remove isolated nodes\n",
    "G.remove_nodes_from(list(nx.isolates(G)))\n",
    "print(\"Total nodes after removing isolates:\", G.number_of_nodes())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use weakly connected components to extract the largest component\n",
    "giant_nodes = max(nx.weakly_connected_components(G), key=len)\n",
    "\n",
    "# Keep only the largest component, copy() preserves node/edge attributes\n",
    "G = G.subgraph(giant_nodes).copy()\n",
    "\n",
    "print(\"Largest component ‚Üí Nodes:\", G.number_of_nodes(), \"| Edges:\", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file to upload to Github\n",
    "nx.write_gexf(G, \"rock_bands.gexf\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
