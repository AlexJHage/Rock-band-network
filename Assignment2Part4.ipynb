{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3489b677-e893-4f87-852d-6c45be75edfe",
   "metadata": {},
   "source": [
    "# Part 4 - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f47204b-f721-4c0e-8489-cbb68828880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import urllib.request\n",
    "import re\n",
    "import networkx as nx\n",
    "import json\n",
    "from networkx.readwrite import json_graph\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import numpy as np\n",
    "import csv\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ce79f-7b01-4528-b33e-eb3de6236d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving the labMIT data - sentiment data\n",
    "def load_labmt_sentiment_from_url(url):\n",
    "    sentiment_dict = {}\n",
    "    response = urllib.request.urlopen(url)\n",
    "    lines = response.read().decode('utf-8').splitlines()\n",
    "    reader = csv.DictReader(lines, delimiter='\\t')\n",
    "\n",
    "    for row in reader:\n",
    "        row = {k.strip(): v for k, v in row.items()}  # Clean header keys\n",
    "        word = row['word'].strip().lower()\n",
    "        try:\n",
    "            score = float(row['happiness_average'])\n",
    "            sentiment_dict[word] = score\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return sentiment_dict\n",
    "\n",
    "# Replace with your actual GitHub raw URL\n",
    "github_url = \"https://raw.githubusercontent.com/AlexJHage/Rock-band-network/main/labMIT.txt\"\n",
    "labmt_dict = load_labmt_sentiment_from_url(github_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9bf971-cd65-4a36-9dd8-9199941cdbc6",
   "metadata": {},
   "source": [
    "In order to determine the sentiment for the graph, three methods are created. \n",
    "1. calculate_labmt_sentiment()\n",
    "\n",
    "2. tokenize()\n",
    "\n",
    "3. annotate_sentiment()\n",
    "- Calls the tokenize method, such that the text for each wikipedia page is tokenized. Then calls the calculate_labmt_sentiment in order to determine the average sentiment of text for each node, this sentiment value is then annotated to the node as an attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98c810c-d6b9-4d03-a97a-b52d495ea45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_labmt_sentiment(tokens, labmt_dict, neutralSentimentDel):\n",
    "    scores = []\n",
    "    for word in tokens:\n",
    "        if word in labmt_dict:\n",
    "            score = labmt_dict[word]\n",
    "            if neutralSentimentDel == 0 or (4 > score or score > 6):\n",
    "                scores.append(score)\n",
    "    return sum(scores) / len(scores) if scores else None\n",
    "\n",
    "def tokenize(text):\n",
    "    # Lowercase and remove non-alphabetic characters\n",
    "    tokens = re.findall(r'\\b[a-z]+\\b', text.lower())\n",
    "    return tokens\n",
    "\n",
    "def annotate_sentiment(G, labmt_dict, neutralSentimentDel):\n",
    "    for node in G.nodes():\n",
    "        content = G.nodes[node].get('text', '')\n",
    "        tokens = tokenize(content)\n",
    "        sentiment = calculate_labmt_sentiment(tokens, labmt_dict, neutralSentimentDel)\n",
    "        G.nodes[node]['sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee79bd-4627-49b6-b70b-f6bcc232944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the actual sentiment\n",
    "annotate_sentiment(G, labmt_dict, neutralSentimentDel = 1)\n",
    "\n",
    "# Extract sentiment scores from graph\n",
    "sentiments = [(n, G.nodes[n].get('sentiment')) for n in G.nodes() if G.nodes[n].get('sentiment') is not None]\n",
    "names, scores = zip(*sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59980db4-6c64-4f21-9b69-3ac46ebe5b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "mean_sentiment = statistics.mean(scores)\n",
    "median_sentiment = statistics.median(scores)\n",
    "variance_sentiment = statistics.variance(scores)\n",
    "percentile_25 = np.percentile(scores, 25)\n",
    "percentile_75 = np.percentile(scores, 75)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Mean sentiment: {mean_sentiment:.3f}\")\n",
    "print(f\"Median sentiment: {median_sentiment:.3f}\")\n",
    "print(f\"Variance: {variance_sentiment:.3f}\")\n",
    "print(f\"25th percentile: {percentile_25:.3f}\")\n",
    "print(f\"75th percentile: {percentile_75:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9011ede8-d04f-4d0c-90ce-409d79798876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(scores, bins=30, kde=True, color='mediumseagreen', edgecolor='black')\n",
    "\n",
    "# Annotate statistics on plot\n",
    "plt.axvline(mean_sentiment, color='blue', linestyle='--', label=f'Mean: {mean_sentiment:.2f}')\n",
    "plt.axvline(median_sentiment, color='red', linestyle='--', label=f'Median: {median_sentiment:.2f}')\n",
    "plt.axvline(percentile_25, color='purple', linestyle=':', label=f'25th %ile: {percentile_25:.2f}')\n",
    "plt.axvline(percentile_75, color='orange', linestyle=':', label=f'75th %ile: {percentile_75:.2f}')\n",
    "\n",
    "plt.title(\"Distribution of Wikipedia Page Sentiment\")\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Number of Artists\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aeae17-fb77-4a52-a169-86e1463a0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find happiest and saddest artists\n",
    "sorted_sentiments = sorted(sentiments, key=lambda x: x[1])\n",
    "saddest = sorted_sentiments[:10]\n",
    "happiest = sorted_sentiments[-10:]\n",
    "\n",
    "print(\"\\nðŸŽ­ Saddest Artists:\")\n",
    "for name, score in saddest:\n",
    "    print(f\"{name}: {score:.2f}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Happiest Artists:\")\n",
    "for name, score in reversed(happiest):\n",
    "    print(f\"{name}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315fc5f7-201a-4268-9dda-d9c4f1a0a9b8",
   "metadata": {},
   "source": [
    "## Sentiment of communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171bd2a9-56cc-47e2-9180-f38ff2d7f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sentiment for the 7 largest communities\n",
    "\n",
    "# Step 1: Select the 7 largest communities - as these are the communities for which we made TF.IDF analysis\n",
    "sorted_communities = sorted(communities, key=len, reverse=True)\n",
    "top_communities = sorted_communities[:7]\n",
    "\n",
    "# Step 2: Calculate average sentiment and name each community\n",
    "community_info = []\n",
    "\n",
    "for i, community in enumerate(top_communities):\n",
    "    # Get sentiment scores\n",
    "    scores = [G.nodes[n]['sentiment'] for n in community if G.nodes[n].get('sentiment') is not None]\n",
    "    avg_sentiment = sum(scores) / len(scores) if scores else None\n",
    "\n",
    "    # Get top 3 most connected bands in the community\n",
    "    subgraph = G.subgraph(community)\n",
    "    top_nodes = sorted(subgraph.degree, key=lambda x: x[1], reverse=True)[:3]\n",
    "    top_band_names = [n for n, _ in top_nodes]\n",
    "\n",
    "    # Store info\n",
    "    community_info.append({\n",
    "        \"index\": i,\n",
    "        \"name\": \", \".join(top_band_names),\n",
    "        \"avg_sentiment\": avg_sentiment,\n",
    "        \"size\": len(community)\n",
    "    })\n",
    "\n",
    "# Step 3: Print community info\n",
    "print(\"\\nðŸŽ¼ Community Sentiment Overview:\")\n",
    "for info in community_info:\n",
    "    print(f\"Community {info['index'] + 1} ({info['name']}):\")\n",
    "    print(f\"  Size: {info['size']}\")\n",
    "    print(f\"  Average Sentiment: {info['avg_sentiment']:.3f}\" if info['avg_sentiment'] is not None else \"  No sentiment data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c978dcb-fe10-46aa-95de-cbd67acbca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Identify happiest and saddest communities\n",
    "valid_communities = [c for c in community_info if c['avg_sentiment'] is not None]\n",
    "sorted_by_sentiment = sorted(valid_communities, key=lambda x: x['avg_sentiment'])\n",
    "\n",
    "saddest = sorted_by_sentiment[:3]\n",
    "happiest = sorted_by_sentiment[-3:]\n",
    "\n",
    "print(\"\\nðŸ˜¢ Saddest Communities:\")\n",
    "for c in saddest:\n",
    "    print(f\"Community {c['index'] + 1} ({c['name']}): {c['avg_sentiment']:.3f}\")\n",
    "\n",
    "print(\"\\nðŸ˜„ Happiest Communities:\")\n",
    "for c in reversed(happiest):\n",
    "    print(f\"Community {c['index'] + 1} ({c['name']}): {c['avg_sentiment']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
